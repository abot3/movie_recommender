---
title: "Project4_6723_botelho3"
author: "botelho3"
date: '2022-12-04'
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    theme: readable
    toc: yes
    toc_float: yes
---


```{r warning=FALSE, include=FALSE}
library(dplyr)
library(lubridate)
library(ggplot2)
library(recommenderlab)
library(DT)
library(data.table)
library(reshape2)
```

## Load Data


**Load Ratings Data**
```{r echo=FALSE}
# use colClasses = 'NULL' to skip columns
ratings = read.csv('ratings.dat', 
                   sep = ':',
                   colClasses = c('integer', 'NULL'), 
                   header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
head(ratings)
```

**Load Movies Data**
```{r echo=FALSE}
movies = readLines('movies.dat')
movies = strsplit(movies, split = "::", fixed = TRUE, useBytes = TRUE)
movies = matrix(unlist(movies), ncol = 3, byrow = TRUE)
movies = data.frame(movies, stringsAsFactors = FALSE)
colnames(movies) = c('MovieID', 'Title', 'Genres')
movies$MovieID = as.integer(movies$MovieID)

# convert accented characters
movies$Title = iconv(movies$Title, "latin1", "UTF-8")

# extract year
movies$Year = as.numeric(unlist(
  lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))
head(movies)
```

**Load User Data**
```{r echo=FALSE}
users = read.csv('users.dat',
                 sep = ':', header = FALSE)
users = users[, -c(2,4,6,8)] # skip columns
colnames(users) = c('UserID', 'Gender', 'Age', 'Occupation', 'Zip-code')
head(users)
```


<!-- ## Exploration -->
<!-- ```{r} -->
<!-- dim(users) -->
<!-- length(unique(ratings$UserID)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- movies_not_rated = movies %>%  -->
<!--   filter(!(MovieID %in% ratings$MovieID)) -->
<!-- dim(movies_not_rated) -->
<!-- ``` -->

<!-- **Distribution of Ratings** -->

<!-- ```{r} -->
<!-- tmp = data.frame(Rating = 1:5,  -->
<!--                  freq = as.vector(table(ratings$Rating)/nrow(ratings))) -->
<!-- ggplot(data = tmp, aes(x = Rating, y = freq)) + -->
<!--   geom_bar(stat="identity", fill = 'steelblue', width = 0.6) +  -->
<!--   geom_text(aes(label=round(freq, dig=2)),  -->
<!--                 vjust=1.6, color="white", size=3.5) + -->
<!--   theme_minimal() -->
<!-- ``` -->


<!-- **Ratings Per User** -->
<!-- ```{r} -->
<!-- tmp = ratings %>%  -->
<!--   group_by(UserID) %>%  -->
<!--   summarize(ratings_per_user = n())  -->
<!--   summary(tmp$ratings_per_user) -->
<!--   stem(tmp$ratings_per_user) -->
<!--   sum(tmp$ratings_per_user > 500) -->
<!--   sort(tmp$ratings_per_user[tmp$ratings_per_user>1300]) -->

<!-- tmp %>% -->
<!--   ggplot(aes(ratings_per_user)) + -->
<!--   geom_bar(fill = "steelblue") + coord_cartesian(c(20, 500)) -->

<!-- #tmp = tmp %>% full_join(users, by = 'UserID') -->
<!-- ``` -->

<!-- **Ratings Per Movie** -->

<!-- ```{r} -->
<!-- tmp = ratings %>%  -->
<!--   group_by(MovieID) %>%  -->
<!--   summarize(ratings_per_movie = n(), ave_ratings = mean(Rating)) %>% -->
<!--   inner_join(movies, by = 'MovieID') -->
<!-- summary(tmp$ratings_per_movie) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- tmp %>%  -->
<!--   filter(ratings_per_movie > 2000) %>% -->
<!--   arrange(desc = ratings_per_movie) %>% -->
<!--   select(c("Title", "ratings_per_movie")) %>% -->
<!--   print(n = 31) -->

<!-- tmp %>% ggplot(aes(ratings_per_movie)) +  -->
<!--   geom_bar(fill = "steelblue", width = 1) + coord_cartesian(c(1,1500)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- small_image_url = "https://liangfgithub.github.io/MovieImages/" -->
<!-- ratings %>%  -->
<!--   group_by(MovieID) %>%  -->
<!--   summarize(ratings_per_movie = n(),  -->
<!--             ave_ratings = round(mean(Rating), dig=3)) %>% -->
<!--   inner_join(movies, by = 'MovieID') %>% -->
<!--   filter(ratings_per_movie > 1000) %>% -->
<!--   top_n(10, ave_ratings) %>% -->
<!--   mutate(Image = paste0('<img src="',  -->
<!--                         small_image_url,  -->
<!--                         MovieID,  -->
<!--                         '.jpg?raw=true"></img>')) %>% -->
<!--   select('Image', 'Title', 'ave_ratings') %>% -->
<!--   arrange(desc(-ave_ratings)) %>% -->
<!--   datatable(class = "nowrap hover row-border",  -->
<!--             escape = FALSE,  -->
<!--             options = list(dom = 't', -->
<!--                           scrollX = TRUE, autoWidth = TRUE)) -->
<!-- ``` -->


**Dist of Genres**

Compute matrix of Genres vs. MovieIDs (rows: Genre, cols: MovieIDs).

```{r}
genres = as.data.frame(movies$Genres, stringsAsFactors=FALSE)
tmp = as.data.frame(tstrsplit(genres[,1], '[|]',
                              type.convert=TRUE),
                    stringsAsFactors=FALSE)
genre_list = c("Action", "Adventure", "Animation", 
               "Children's", "Comedy", "Crime",
               "Documentary", "Drama", "Fantasy",
               "Film-Noir", "Horror", "Musical", 
               "Mystery", "Romance", "Sci-Fi", 
               "Thriller", "War", "Western")
m = length(genre_list)
genre_matrix = matrix(0, nrow(movies), length(genre_list))
for(i in 1:nrow(tmp)){
  genre_matrix[i,genre_list %in% tmp[i,]]=1
}
colnames(genre_matrix) = genre_list
remove("tmp", "genres")
```

```{r, echo=FALSE}
data.frame(Genres = genre_list, 
                 Freq = as.vector(colMeans(genre_matrix))) %>% 
  ggplot(aes(reorder(Genres, Freq), Freq, fill = Freq)) + 
  geom_bar(stat = "identity") + 
    geom_text(aes(label = round(Freq, dig=2)), 
            position = position_stack(vjust = 0.5), 
            color="white", size=3) + 
  coord_flip() + 
  scale_colour_brewer(palette="Set1") + 
  labs(y = 'Frequency', x = 'Genre')
```


<!-- ```{r} -->
<!-- tmp = ratings %>%  -->
<!--   left_join(data.frame(MovieID = movies$MovieID, genre_matrix),  -->
<!--             by = "MovieID") %>% -->
<!--   select(-c("UserID", "MovieID", "Rating", "Timestamp")) -->
<!-- data.frame(Genres = genre_list,  -->
<!--            Popularity = as.vector(colMeans(tmp))) %>%  -->
<!--   ggplot(aes(reorder(Genres, Popularity), Popularity, fill = Popularity)) +  -->
<!--   geom_bar(stat = "identity") +  -->
<!--   geom_text(aes(label = round(Popularity, dig=3)),  -->
<!--             position = position_stack(vjust = 0.5),  -->
<!--             color="white", size=3) +  -->
<!--   coord_flip() +  -->
<!--   labs(y = 'Popularity', x = 'Genre') -->
<!-- ``` -->

<!-- ```{r} -->
<!-- tmp = rowSums(genre_matrix) -->
<!-- summary(tmp) -->

<!-- movies[which(tmp==6), ] -->
<!-- movies[which(tmp==5), ] -->
<!-- ``` -->


<!-- ```{r} -->
<!-- # range(movies$Year) % 1919 to 2000 -->
<!-- tmp = data.frame(Year = movies$Year, genre_matrix) %>% -->
<!--   group_by(Year) %>% -->
<!--   summarise_all(sum) -->
<!-- tmp[,-1] = apply(tmp[, -1], 2, cumsum) -->
<!-- tmp[,-1] = tmp[,-1]/sum(tmp[nrow(tmp), -1]) -->
<!-- print(round(tmp[nrow(tmp),-1], dig=3)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- tmp = reshape2::melt(tmp, id.vars="Year")  -->
<!-- tmp %>% -->
<!--   ggplot(aes(Year, value, group = variable)) + -->
<!--   geom_area(aes(fill = variable)) +  -->
<!--   geom_line(aes(group = variable), position = "stack") -->
<!-- ``` -->






# System 1 - Recommendation Based on Genres

In this section 2 recommendation algorithms are proposed. Both algorithms take
a list of genres as input and produce a ranking of the best movies in that genre
based on the corpus. The corpus is a collection of ratings by users on the site
Movielens.org from 2000 to 2003.

```{r include=FALSE}
movies = readLines('movies.dat')
movies = strsplit(movies, split = "::", fixed = TRUE, useBytes = TRUE)
movies = matrix(unlist(movies), ncol = 3, byrow = TRUE)
movies = data.frame(movies, stringsAsFactors = FALSE)
colnames(movies) = c('MovieID', 'Title', 'Genres')
movies$MovieID = as.integer(movies$MovieID)

# convert accented characters
movies$Title[73]
movies$Title = iconv(movies$Title, "latin1", "UTF-8")
movies$Title[73]

# extract year
movies$Year = as.numeric(unlist(
  lapply(movies$Title, function(x) substr(x, nchar(x)-4, nchar(x)-1))))
```

```{r include=FALSE}
users = read.csv('users.dat',
                 sep = ':', header = FALSE)
users = users[, -c(2,4,6,8)] # skip columns
colnames(users) = c('UserID', 'Gender', 'Age', 'Occupation', 'Zip-code')
```


## Recommendation Algorithm 1

The first algorithm simply returns the top 5 movies in each genre, ranked in
descending order based on their average rating.

```{r}
checkbox_genres = c(genre_list[1], genre_list[8], genre_list[length(genre_list)])
```
```{r echo=FALSE}
checkbox_genres
```

```{r}
system_1_recommend <- function(ratings, movies,
                     genre_list, genre_matrix,
                     checkbox_genres) {
  num_genres = length(checkbox_genres)
  TOP_N = 6
  joined = ratings %>% 
    group_by(MovieID) %>% 
    summarize(ratings_per_movie = n(), ave_ratings = mean(Rating))
  joined = movies %>%
    left_join(joined, by = 'MovieID', all.x = TRUE)
 
  # Transpose the relevant genres. 
  row_idx = which(genre_list %in% checkbox_genres) 
  genre_names = genre_list[row_idx]
  tgm = t(genre_matrix)
  tgm = tgm[row_idx, ]

  result_cols =  c("MovieID", "ave_ratings", "Genres", "Genre") 
  results_list = vector("list", num_genres) 
  names(results_list) = checkbox_genres
  results = data.frame(matrix(nrow = num_genres, ncol = 4))
  colnames(results) <- result_cols
  for (i in 1:length(row_idx)) {
    movies_row_idx = which(tgm[i, ] == 1)
    tmp = joined %>%
            slice(movies_row_idx) %>%
            mutate(mean_ratings_per_movie = mean(ratings_per_movie, na.rm = TRUE)) %>%
            filter(ratings_per_movie > mean_ratings_per_movie) %>%
            arrange(desc(ave_ratings)) 
    s = (i-1) * TOP_N + 1
    e = s + TOP_N - 1
    c = result_cols[!(result_cols %in% c("Genre"))]
    results[s:e, c] = tmp[1:TOP_N, c]
    results[s:e, ]$Genre = genre_names[i]
    results_list[[i]] = tmp$MovieID[1:TOP_N]
  }
  return(results)
}

system_1_results = system_1_recommend(ratings, movies,
                                      genre_list, genre_matrix, checkbox_genres)

# Display results
for (g in checkbox_genres) {
  print(system_1_results[system_1_results$Genre == g, ])
}
```


## Recommendation Algorithm 2

Unlike algorithm 1 above I want this algorithm to attempt to consider which
movies are *trending*. Trending in this case should mean that the `trending`
movies are receiving relatively more attention (ratings) in the last year. Since
we only have a dataset of ratings per movie, not e.g. downloads or ticket sales
per movie, we will use the number of ratings and the rating values as a proxy
for popularity. Similar to algorithm one we will try to incorporate genre 
restrictions.

In order to determine whether or not a movie is receiving more attention in
recent years we will need to normalize the number of ratings based on the
year of the rating, and the year of the movie.

<!-- Most popular movies in the genre. -->

<!-- - Highest number of average ratings.  -->
<!-- - Discount factor for age. Older movies are likely to have more ratings? So -->
<!-- discount them. -->


```{r echo=FALSE}
# Add a ratings_per_movie and ave_ratings column
tmp = ratings %>% 
  group_by(MovieID) %>% 
  summarize(ratings_per_movie = n(), ave_ratings = mean(Rating)) %>%
  inner_join(movies, by = 'MovieID')

# Add a ratings_per_movie_year column
tmp = ratings %>% 
  inner_join(movies, by = 'MovieID') %>%
  group_by(Year) %>% 
  mutate(ratings_per_movie_year = n()) %>%
  group_by(MovieID) %>%
  summarize(ratings_per_movie = n(), ave_ratings = mean(Rating),
            ratings_per_movie_year = first(ratings_per_movie_year), Year = first(Year),
            Title = first(Title))

tmp %>%
  filter(ratings_per_movie > 2000) %>%
  arrange(desc(ratings_per_movie)) %>%
  select(c("Title", "ratings_per_movie", "ratings_per_movie_year", "Year")) %>%
  head()

# unique((tmp %>% group_by(ratings_per_movie))$ratings_per_movie)
# unique((tmp %>% group_by(ratings_per_movie_year))$ratings_per_movie_year)
```

Plot histogram of ratings_per_movie. Very skewed towards individual movies with
a lot of ratings. We need to normalize the popularity of movies in some way, to
be able to compare popularity across movies.

```{r echo=FALSE}
tmp %>%
  group_by(ratings_per_movie) %>%
  mutate(count = n()) %>%
  ggplot(., aes(x = ratings_per_movie, y = count)) + 
  geom_bar(stat='identity', fill = "steelblue", width = 1) + coord_cartesian(c(20, 500))
```

Plot histogram of movies released per year. The movies in the dataset are skewed
towards the year 2000. A recommendation algorithm may wish to normalize the data
in some way relative to the number of movies released in the same (or neighboring)
years.

In our case the popularity of a movie from a given year could be defined as the
percent of the mean ratings for a movie produced in that year
`pct = #_ratings_movie_a_in_yr_2000 / mean_ratings_of_movies_in_year_2000`.

```{r echo=FALSE}
#  # Same as above just different plotting method.
# tmp %>%
#   group_by(ratings_per_movie) %>%
#   ggplot(., aes(ratings_per_movie)) + 
#   geom_bar(fill = "steelblue", width = 1) + coord_cartesian(c(20, 500))

# Plot histogram of movies per year
tmp %>%
  group_by(Year) %>%
  ggplot(aes(Year)) + 
    geom_bar(fill = "steelblue", width = 1) + coord_cartesian(c(1900,2020))

```

Plot histogram of reviews_per_movie_year. All ratings in the dataset are from 2000-2004.
Again, like movies released per year above,
the number of reviews is skewed towards recent movies (movies released close to 2000).
A good recommendation algorithm may wish to choose an algorithm that considers
the total number of ratings for the average movie in a similar year, E.g.
considering a weighted mean of ratings in a sliding 5 year window.


In our case the popularity of a movie from a given year could be defined as the
percent of the mean ratings in that year`pct = #_ratings_movie_a_in_yr_2000 / mean_movie_ratings_in_year_2000`.

```{r echo=FALSE}
tmp %>%
  group_by(Year) %>%
  summarize(ratings_per_movie_year = first(ratings_per_movie_year)) %>%
  ggplot(aes(x = Year, y = ratings_per_movie_year)) + 
    geom_bar(stat='identity', fill = "steelblue", width = 1) + coord_cartesian(c(1900,2020))
```

- Add a ratings_per_movie and ave_ratings column.
- Add a ratings_per_year column. The number of ratings in a given year.

```{r}
# Add a ratings_per_movie and ave_ratings column
tmp = ratings %>% 
  group_by(MovieID) %>% 
  summarize(ratings_per_movie = n(), ave_ratings = mean(Rating)) %>%
  inner_join(movies, by = 'MovieID')

# Add a ratings_per_year column
tmp = ratings %>% 
  inner_join(movies, by = 'MovieID') %>%
  group_by(Year) %>% 
  mutate(ratings_per_movie_year = n()) %>%
  ungroup %>%
  group_by(MovieID) %>%
  summarize(ratings_per_movie = n(), ave_ratings = mean(Rating),
            ratings_per_movie_year = first(ratings_per_movie_year), Year = first(Year),
            Title = first(Title))

# Add a ratings_per_last_5_yr
last_year = 2003
ratings = ratings %>% 
  mutate(RYear = year(as_datetime(Timestamp)))
  # group_by(RYear) %>%
  # mutate(ratings_per_year = n())
head(ratings)
head(tmp)
```

How many ratings are there in each of the 4 years in the dataset? Year 2000 has
more ratings than all other years combined. So to measure `trendiness` or popularity
let's compare rating behaviours in `[2001, 2002, 2003]` against the baseline `[2000]`.
Let's define popular movies as movies that received more ratings in 2001-2003
than they did in 2000. This comparison requires the number of ratings to be
normalized by the number of ratings in the two time windows. We should also 
normalize by the year the movies were released as older movies receive less ratings
regardless of whether we're looking at ratings data between 2001-2003 or 2004, i.e.
there are less reviewers of 1950s movies than 1990s movies in the 200s.

```{r}
rpy = rep(0, 4)
rpy[1] = length(which(ratings$RYear == 2000))
rpy[2] = length(which(ratings$RYear == 2001))
rpy[3] = length(which(ratings$RYear == 2002))
rpy[4] = length(which(ratings$RYear == 2003))
rpy
```

Putting it all together.

```{r}
# Create a data set of movies in year [2000].
# Compute the ratings per movie, ratings per production year.
prev = ratings %>%
  filter(RYear < 2001) %>%
  inner_join(movies, by = 'MovieID') %>%
  group_by(MovieID) %>%
  mutate(ratings_per_movie = n(),
         ave_ratings = mean(Rating)) %>%
  ungroup %>%
  group_by(Year) %>% 
  mutate(ratings_per_movie_year = n()) %>%
  ungroup()
prev = prev %>%
  mutate(pct_ratings_per_movie_year =  ratings_per_movie / ratings_per_movie_year)

# Create a data set of movies in year [2001, 2002, 2003] 
# Compute the ratings per movie, ratings per production year.
now = ratings %>%
  filter(RYear >= 2001) %>%
  inner_join(movies, by = 'MovieID') %>%
  group_by(MovieID) %>%
  mutate(ratings_per_movie = n(),
         ave_ratings = mean(Rating)) %>%
  ungroup %>%
  group_by(Year) %>% 
  mutate(ratings_per_movie_year = n()) %>%
  ungroup()
now = now %>%
  mutate(pct_ratings_per_movie_year =  ratings_per_movie / ratings_per_movie_year)
```

Now we can compute 4 metrics and add them as columns, i.e. we will compute these
columns for each unique MovieID (row) in the matrix.

- ratings_per_movie: # of ratings for each movie.
- ratings_per_movie_year: # of all ratings of all movies produced in year X. E.g. X = 1959.
- pct_ratings_per_movie_year: % of all ratings of all movies produced in year X. E.g. X = 1959.
- ave_ratings: mean rating for each movie.

```{r}
now = now %>% 
  group_by(MovieID) %>%
  summarize(
    MovieID = first(MovieID),
    Rating = mean(Rating),
    ratings_per_movie = first(ratings_per_movie),
    ratings_per_movie_year = first(ratings_per_movie_year),
    pct_ratings_per_movie_year = first(pct_ratings_per_movie_year),
    ave_ratings = first(ave_ratings),
  )
  
prev = prev %>% 
  group_by(MovieID) %>%
  summarize(
    MovieID = first(MovieID),
    Title = first(Title),
    Rating = mean(Rating),
    Year = first(Year),
    RYear = first(RYear),
    ratings_per_movie = first(ratings_per_movie),
    ratings_per_movie_year = first(ratings_per_movie_year),
    pct_ratings_per_movie_year = first(pct_ratings_per_movie_year),
    ave_ratings = first(ave_ratings),
  )

# Join now (2001-2003) and prev (2000) datasets on MovieID.
# Now we have 2 copies of the  ratings_per_movie and pct_ratings_per_movie_year
# columns, 1 from each datset.
a =  prev %>%
  full_join(now, by = 'MovieID')
```

Now that we have some metrics defined we can compare the metrics across the two
time periods (`[2000], [2001, 2003]`), to determine the trendiest movies.

Again we can filter by genre first.

```{r}
checkbox_genres = c(genre_list[1], genre_list[8], genre_list[length(genre_list)])
```
```{r echo=FALSE}
checkbox_genres
```


```{r}
row_idx = which(genre_list %in% checkbox_genres) 
tgm = t(genre_matrix)
tgm = tgm[row_idx, ]

# result_cols =  c("MovieID", "ave_ratings", "Genres", "Genre") 
# results_list = vector("list", num_genres) 
# names(results_list) = checkbox_genres
# results = data.frame(matrix(nrow = num_genres, ncol = 4))
# colnames(results) <- result_cols
results = c()
for (i in 1:length(row_idx)) {
  movies_row_idx = which(tgm[i, ] == 1)
  results = union(results, movies$MovieID[movies_row_idx])
}

a = a %>%
    filter(MovieID %in% results)
head(a)
```

Now we can diff the metrics between period `x = 2000`  and `y = [2001, 2002, 2003]`.
I tried 5 different algorithms.
1. ratio of percent of all ratings for movies produced in a year Z in x vs y, `pct_y / pct_x`.
An increase in this ratio indicates trendiness.
2. difference in percent of all ratings for movies produced in a year Z in x vs y, `pct_y - pct_x`.
An increase in this difference indicates trendiness.
3. ratio of ratings for a *single* movie in period y vs. x, `ratings_y / ratings_x`.
4. difference of ratings for a *single* movie in period y vs. x, `ratings_y - ratings_x`.
5. difference of normalized (centered, scaled) ratings for a *single* movie in period y vs. x,
`scale(ratings_y) - scale(ratings_x)`.
6. ratio of normalized (centered, scaled) ratings for a *single* movie in period y vs. x,
`scale(ratings_y) / scale(ratings_x)`.

```{r}
# Now we can compare the 
a = a %>%
  mutate(ratio = pct_ratings_per_movie_year.y / pct_ratings_per_movie_year.x) %>%
  mutate(diff = pct_ratings_per_movie_year.y - pct_ratings_per_movie_year.x) %>%
  mutate(ratio2 = ratings_per_movie.y / ratings_per_movie.x) %>%
  mutate(diff2 = ratings_per_movie.y - ratings_per_movie.x) %>%
  mutate(metric1 = scale(ratings_per_movie.y) - scale(ratings_per_movie.x)) %>%
  mutate(metric2 = scale(ratings_per_movie.y) / scale(ratings_per_movie.x))

to_order = a %>%
  select(MovieID, Title, ratio, diff, ratio2, diff2, metric1, metric2, Year, Rating.x) %>%
  filter(Rating.x > 4.0) %>%
  arrange(desc(metric1))
```

I inspected the results manually and rated the 5 methods subjectively based on
my own impression of which movies I recognized as being popular in the 80s and 90s.
I found that in my opinion the 5th method, the normalized difference, performed
the best. Looking at the results below they're highly rated recognizable
movies. Also of note is that the recommendations are not ranked in order of e.g.
Year or Rating, so the algorithm is incorporating temporal information.

This algorithm is a promising candidate for creating a trending algorithm
that recommends relevant movies based on recent ratings activity.

```{r}
head(to_order, n=30)
```


```{r eval=FALSE, include=FALSE}
#head(tmp, n = 30)
head(prev[!duplicated(prev$MovieID), ], n = 30)
```




# System 2 - Collaborative Recommendation System

Generating a sparse matrix where each row represents a unique user and each
column represents a unique item (movie). Slicing by row gives the ratings of
each item for a single user.

```{r message=FALSE}
set.seed(6723)
library(recommenderlab)
library(proxy)
library(testit)
ratings = read.csv('ratings.dat', 
                   sep = ':',
                   colClasses = c('integer', 'NULL'), 
                   header = FALSE)
colnames(ratings) = c('UserID', 'MovieID', 'Rating', 'Timestamp')
i = paste0('u', ratings$UserID)
j = paste0('m', ratings$MovieID)
x = ratings$Rating
tmp = data.frame(i, j, x, stringsAsFactors = T)
Rmat = sparseMatrix(as.integer(tmp$i), as.integer(tmp$j), x = tmp$x)
rownames(Rmat) = levels(tmp$i)
colnames(Rmat) = levels(tmp$j)
M = dim(Rmat)[1]  # users
N = dim(Rmat)[2]  # ratings
Rmat = new('realRatingMatrix', data = Rmat)

train = Rmat[1:500, ]
test = Rmat[501, ]
```



### UBCF Recommendation

The following is an implementation of User-Based Collaborative Filtering.

**My Implementation**

```{r}
set.seed(6723)
nn = 20
M = 500
N = N

# center
newdata = as(test, "matrix")
newuser.means = rowMeans(newdata, na.rm = TRUE)
newdata = newdata - newuser.means

data = as(train, "matrix")
user.means = rowMeans(data, na.rm = TRUE)
data = data - user.means
assert(dim(data) == c(M, N))

sim1 = proxy::simil(data, newdata, method = "cosine")
sim1 = (1 + sim1)/2
class(sim1)

sim1_mat <- `dim<-`(c(sim1), dim(sim1))
knn_rows = head(order(sim1[,1], decreasing = TRUE), nn)
sim_df = data.frame(
    id = rownames(sim1)[knn_rows],
    x = sim1_mat[knn_rows, 1])

# 20x3706 i.e. nn x N
knn_mat = data[knn_rows,]
assert(dim(knn_mat)[1] == nn)

# nnx1 % t(N,1) -> nn x N i.e. 20 x 3706
# Note similarity matrix might have NA in it.
s_coeff_mat = as.matrix(sim_df$x) %*% t(rep(1, N))

# Zero out the coefficient wherever the k nearest neighbors are NA.
denom_mat = s_coeff_mat * as.integer(!is.na(knn_mat))

 r = denom_mat * knn_mat
# nn x N -> vec len N
r = colSums(r, na.rm = TRUE) / colSums(denom_mat, na.rm = FALSE)
assert(length(r) == N)
mypred = r

# Clean up mypred
test_user_watched = !is.na(newdata)
assert(length(test_user_watched) == N)
mypred = mypred + newuser.means
mypred[which(is.infinite(mypred))] = NA
mypred[which(test_user_watched)] = NA
```


**Comparison to Recommenderlab**

```{r}
recommender.UBCF <- Recommender(train, method = "UBCF",
                                parameter = list(normalize = 'center', 
                                                 method = 'Cosine', 
                                                 nn = 20))

p.UBCF <- predict(recommender.UBCF, test, type="ratings")
p.UBCF <- as.numeric(as(p.UBCF, "matrix"))

sum(is.na(p.UBCF) != is.na(mypred)) ### should be zero
max(abs(p.UBCF - mypred), na.rm = TRUE)  ### should be less than 1e-06 
```






### IBCF Recommendation

The following is an implementation of Item-Based Collaborative Filtering.

**My Implementation**

```{r}
# Check this post https://campuswire.com/c/G3D46BBBA/feed/1326
# And this post https://campuswire.com/c/G3D46BBBA/feed/1334.
set.seed(6723)
# center
k = 30
M = 500
N = N

newdata = as(test, "matrix")
newuser.means = mean(newdata, na.rm = TRUE)
newdata = newdata - newuser.means

data = as(train, "matrix")
user.means = rowMeans(data, na.rm = TRUE)
data = data - user.means
assert(dim(data) == c(M, N))

active_items_mask = !is.na(newdata)
active_items_idx = which(active_items_mask)

sim1 = proxy::simil(t(data), method = "cosine")
assert(dim(sim1) == c(N,N))
sim1 = (1 + sim1)/2
class(sim1)

# sim1_mat <- `dim<-`(c(sim1), dim(sim1))
sim1_mat = as.matrix(sim1)

# Remove the diag because they're all ones, which would mess up the ordering
# See office hours https://mediaspace.illinois.edu/media/t/1_2acmsvby 1:22:19
# By default empty entries in the similarity matrix should be NA not 0.
diag(sim1_mat) = NA
s_mat = matrix(NA, nrow = nrow(sim1_mat), ncol = ncol(sim1_mat))
for (row in 1:nrow(sim1_mat)) {
  col_indices = head(order(sim1_mat[row,], decreasing = TRUE), k)
  col_indices = tail(order(sim1_mat[row,], na.last=FALSE, decreasing = FALSE), k)
  s_mat[row, col_indices] = sim1_mat[row, col_indices]
}
assert(dim(s_mat) == c(N,N))
tmp = !is.na(s_mat)
all(s_mat[tmp] == sim1_mat[tmp])

# Keep only k nearest neighbor rows.
# This is going to bring NAs into the matrix
newdata_prime = t(newdata)
r = t(t(s_mat) * as.vector(newdata_prime))
r = rowSums(r, na.rm = TRUE)
r = r / rowSums(s_mat[,active_items_idx], na.rm = TRUE)

# nn x N -> vec len N
assert(length(r) == N)
mypred = r

# Clean up mypred
mypred = mypred + newuser.means
mypred[which(is.infinite(mypred))] = NA
mypred[active_items_idx] = NA
```


**Comparison to Recommenderlab**

```{r}
recommender.IBCF <- Recommender(train, method = "IBCF",
                                parameter = list(normalize = 'center', 
                                                 method = 'Cosine', 
                                                 k = 30))

p.IBCF <- predict(recommender.IBCF, test, type="ratings")
p.IBCF <- as.numeric(as(p.IBCF, "matrix"))

## first output: should be less than 10
sum(is.na(p.IBCF) != is.na(mypred))  

## second output: should be less than 10%
mydiff = abs(p.IBCF - mypred)
sum(mydiff[!is.na(mydiff)] > 1e-6) / sum(!is.na(mydiff)) 
```


**Question: why do we encounter such a big discrepancy for IBCF, but not for
UBCF?**

IBCF results depend on the ordering of the sorted nearest neighbors when there
is a tie between two ratings. The different ordering of nearest neighbors
keeps a different column in the items matrix and later when summing over the rows
to form the prediction for a new user we get a slightly different result.










